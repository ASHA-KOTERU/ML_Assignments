{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asha/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names=glob.glob(\"./data/train/*.jpg\")\n",
    "file_names.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10222"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=pd.read_csv(\"./data/labels.csv\",delimiter=\",\")\n",
    "y_true=y_train['breed']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breed_to_numbers(y_train):\n",
    "    unq=np.unique(y_train)\n",
    "    n=len(unq)\n",
    "    numbers = list(range(0, n))\n",
    "    y_dict=dict(zip(unq,numbers))\n",
    "    y_true=[]\n",
    "    for label in y_train:\n",
    "        n=y_dict[label]\n",
    "        y_true.append(n)\n",
    "    return y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true=breed_to_numbers(y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide the data in to training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split( file_names, y_true, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8177\n",
      "2045\n",
      "8177\n",
      "2045\n"
     ]
    }
   ],
   "source": [
    "print len(X_train)\n",
    "print len(X_test)\n",
    "print len(y_train)\n",
    "print len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_function(filename, label):\n",
    "    image_string = tf.read_file(filename)\n",
    "\n",
    "    # Don't use tf.image.decode_image, or the output shape will be undefined\n",
    "    image = tf.image.decode_jpeg(image_string, channels=3)\n",
    "\n",
    "    # This will convert to float values in [0, 1]\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "\n",
    "    image = tf.image.resize_images(image, [224,224])\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label=tf.one_hot(y_train,120)\n",
    "dataset=tf.data.Dataset.from_tensor_slices((X_train,label))\n",
    "dataset=dataset.map(parse_function)\n",
    "dataset=dataset.batch(batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "def train_data():\n",
    "    x,y = iterator.get_next()\n",
    "    return x,y\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test=tf.one_hot(y_test,120)\n",
    "dataset_test=tf.data.Dataset.from_tensor_slices((X_test,label_test))\n",
    "dataset_test=dataset_test.map(parse_function)\n",
    "dataset_test=dataset_test.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator_test = dataset_test.make_initializable_iterator()\n",
    "def test_data():  \n",
    "    x,y = iterator_test.get_next()\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.bool)  #placeholder for a single boolean value\n",
    "x,y= tf.cond(tf.equal(a, tf.constant(True)), lambda:train_data(), lambda:test_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-16-0539a2b064fc>:30: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    " \n",
    "#convolution, relu and max poling for layer 1\n",
    "conv1=tf.layers.conv2d(x,32,3,strides=1,activation=tf.nn.relu)  \n",
    "conv1=tf.layers.max_pooling2d(conv1,2,2,padding='SAME')\n",
    "\n",
    "conv1=tf.layers.batch_normalization(conv1)\n",
    "\n",
    "    \n",
    "#convolution, relu and max poling for layer 2\n",
    "conv2=tf.layers.conv2d(conv1,64,5,strides=1,activation=tf.nn.relu)  \n",
    "conv2=tf.layers.max_pooling2d(conv2,2,2,padding='SAME')\n",
    "conv2=tf.layers.batch_normalization(conv2)\n",
    "\n",
    "\n",
    "conv3=tf.layers.conv2d(conv2,128,5,strides=1,activation=tf.nn.relu)  \n",
    "conv3=tf.layers.max_pooling2d(conv3,2,2,padding='SAME')\n",
    "\n",
    "conv3=tf.layers.batch_normalization(conv3)\n",
    "\n",
    "\n",
    "fc1=tf.contrib.layers.flatten(conv3)\n",
    "#fully connected layer\n",
    "fc1=tf.layers.dense(inputs=fc1, units=2048,activation=tf.nn.relu)\n",
    "fc2=tf.layers.dense(inputs=fc1, units=512,activation=tf.nn.relu)\n",
    "logits=tf.layers.dense(inputs=fc2, units=120)\n",
    "#softmax for probabilities\n",
    "#prob=tf.nn.softmax(logits)\n",
    "    \n",
    "#calculating loss\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "    \n",
    "#calculating accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n",
      "63\n"
     ]
    }
   ],
   "source": [
    "batches=len(X_train)/batch_size\n",
    "batches=int(batches)\n",
    "epochs=4\n",
    "print batches\n",
    "batches_test=len(X_test)/batch_size\n",
    "print batches_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 running\n",
      "batch 0 loss: 4.7949877 accuracy 0.0\n",
      "batch 10 loss: 4.7445507 accuracy 0.03125\n",
      "batch 20 loss: 4.778106 accuracy 0.03125\n",
      "batch 30 loss: 4.7881536 accuracy 0.0\n",
      "batch 40 loss: 4.7975674 accuracy 0.0625\n",
      "batch 50 loss: 4.793742 accuracy 0.0\n",
      "batch 60 loss: 4.7747984 accuracy 0.03125\n",
      "batch 70 loss: 4.796772 accuracy 0.0\n",
      "batch 80 loss: 4.770176 accuracy 0.03125\n",
      "batch 90 loss: 4.782983 accuracy 0.09375\n",
      "batch 100 loss: 4.7494564 accuracy 0.03125\n",
      "batch 110 loss: 4.808174 accuracy 0.03125\n",
      "batch 120 loss: 4.7425528 accuracy 0.0\n",
      "batch 130 loss: 4.7932897 accuracy 0.03125\n",
      "batch 140 loss: 4.696476 accuracy 0.09375\n",
      "batch 150 loss: 4.5091996 accuracy 0.09375\n",
      "batch 160 loss: 4.650653 accuracy 0.09375\n",
      "batch 170 loss: 4.707104 accuracy 0.03125\n",
      "batch 180 loss: 4.6074796 accuracy 0.0625\n",
      "batch 190 loss: 4.7390623 accuracy 0.0\n",
      "batch 200 loss: 4.6320896 accuracy 0.03125\n",
      "batch 210 loss: 4.610182 accuracy 0.0\n",
      "batch 220 loss: 4.5299673 accuracy 0.0625\n",
      "batch 230 loss: 4.7855096 accuracy 0.0\n",
      "batch 240 loss: 4.6063824 accuracy 0.0\n",
      "batch 250 loss: 4.459318 accuracy 0.0625\n",
      "Epoch: 0 ended\n",
      "Epoch: 1 running\n",
      "batch 0 loss: 4.414407 accuracy 0.03125\n",
      "batch 10 loss: 4.382435 accuracy 0.09375\n",
      "batch 20 loss: 4.5088115 accuracy 0.03125\n",
      "batch 30 loss: 4.4678087 accuracy 0.03125\n",
      "batch 40 loss: 4.611994 accuracy 0.03125\n",
      "batch 50 loss: 4.6889205 accuracy 0.03125\n",
      "batch 60 loss: 4.3100476 accuracy 0.0\n",
      "batch 70 loss: 4.4812107 accuracy 0.0\n",
      "batch 80 loss: 4.493641 accuracy 0.03125\n",
      "batch 90 loss: 4.343986 accuracy 0.0625\n",
      "batch 100 loss: 4.4336443 accuracy 0.0\n",
      "batch 110 loss: 4.433544 accuracy 0.03125\n",
      "batch 120 loss: 4.583249 accuracy 0.0\n",
      "batch 130 loss: 4.5911493 accuracy 0.0625\n",
      "batch 140 loss: 4.4307113 accuracy 0.03125\n",
      "batch 150 loss: 3.9013424 accuracy 0.15625\n",
      "batch 160 loss: 4.0512295 accuracy 0.15625\n",
      "batch 170 loss: 4.1011753 accuracy 0.09375\n",
      "batch 180 loss: 4.038599 accuracy 0.1875\n",
      "batch 190 loss: 4.103113 accuracy 0.125\n",
      "batch 200 loss: 3.8845625 accuracy 0.1875\n",
      "batch 210 loss: 4.015974 accuracy 0.125\n",
      "batch 220 loss: 3.829988 accuracy 0.21875\n",
      "batch 230 loss: 4.085369 accuracy 0.0625\n",
      "batch 240 loss: 3.9587078 accuracy 0.15625\n",
      "batch 250 loss: 3.5571342 accuracy 0.15625\n",
      "Epoch: 1 ended\n",
      "Epoch: 2 running\n",
      "batch 0 loss: 3.5492234 accuracy 0.15625\n",
      "batch 10 loss: 3.7148871 accuracy 0.21875\n",
      "batch 20 loss: 3.5949597 accuracy 0.21875\n",
      "batch 30 loss: 3.6454115 accuracy 0.15625\n",
      "batch 40 loss: 3.996911 accuracy 0.1875\n",
      "batch 50 loss: 3.9909966 accuracy 0.125\n",
      "batch 60 loss: 3.482511 accuracy 0.125\n",
      "batch 70 loss: 3.6771479 accuracy 0.125\n",
      "batch 80 loss: 3.4775226 accuracy 0.15625\n",
      "batch 90 loss: 3.6040425 accuracy 0.1875\n",
      "batch 100 loss: 3.651085 accuracy 0.15625\n",
      "batch 110 loss: 3.4803371 accuracy 0.28125\n",
      "batch 120 loss: 3.9627032 accuracy 0.0625\n",
      "batch 130 loss: 3.5312526 accuracy 0.21875\n",
      "batch 140 loss: 3.6610498 accuracy 0.1875\n",
      "batch 150 loss: 2.798335 accuracy 0.3125\n",
      "batch 160 loss: 2.9522538 accuracy 0.34375\n",
      "batch 170 loss: 3.1093044 accuracy 0.28125\n",
      "batch 180 loss: 3.0090065 accuracy 0.3125\n",
      "batch 190 loss: 2.215717 accuracy 0.53125\n",
      "batch 200 loss: 2.6235638 accuracy 0.4375\n",
      "batch 210 loss: 2.572795 accuracy 0.4375\n",
      "batch 220 loss: 2.350893 accuracy 0.4375\n",
      "batch 230 loss: 2.7836423 accuracy 0.375\n",
      "batch 240 loss: 2.3276057 accuracy 0.4375\n",
      "batch 250 loss: 1.6059912 accuracy 0.71875\n",
      "Epoch: 2 ended\n",
      "Epoch: 3 running\n",
      "batch 0 loss: 1.5810463 accuracy 0.59375\n",
      "batch 10 loss: 2.084817 accuracy 0.53125\n",
      "batch 20 loss: 1.7615752 accuracy 0.65625\n",
      "batch 30 loss: 1.7170421 accuracy 0.5\n",
      "batch 40 loss: 2.1786914 accuracy 0.53125\n",
      "batch 50 loss: 2.3686824 accuracy 0.4375\n",
      "batch 60 loss: 1.7614793 accuracy 0.6875\n",
      "batch 70 loss: 1.5518646 accuracy 0.625\n",
      "batch 80 loss: 1.4181955 accuracy 0.65625\n",
      "batch 90 loss: 1.5730605 accuracy 0.5625\n",
      "batch 100 loss: 1.834487 accuracy 0.65625\n",
      "batch 110 loss: 1.599471 accuracy 0.59375\n",
      "batch 120 loss: 1.5421561 accuracy 0.6875\n",
      "batch 130 loss: 1.8119632 accuracy 0.59375\n",
      "batch 140 loss: 1.8406514 accuracy 0.53125\n",
      "batch 150 loss: 0.8833983 accuracy 0.8125\n",
      "batch 160 loss: 1.4208789 accuracy 0.78125\n",
      "batch 170 loss: 1.2340696 accuracy 0.6875\n",
      "batch 180 loss: 0.78891873 accuracy 0.84375\n",
      "batch 190 loss: 0.7932956 accuracy 0.875\n",
      "batch 200 loss: 0.92714816 accuracy 0.71875\n",
      "batch 210 loss: 0.8529366 accuracy 0.8125\n",
      "batch 220 loss: 0.98015183 accuracy 0.71875\n",
      "batch 230 loss: 1.010959 accuracy 0.78125\n",
      "batch 240 loss: 0.35096186 accuracy 0.96875\n",
      "batch 250 loss: 0.22473317 accuracy 1.0\n",
      "Epoch: 3 ended\n",
      "optimazion finished\n",
      "batch 0 accuracy: 0.0\n",
      "batch 10 accuracy: 0.0625\n",
      "batch 20 accuracy: 0.0\n",
      "batch 30 accuracy: 0.0625\n",
      "batch 40 accuracy: 0.0625\n",
      "batch 50 accuracy: 0.09375\n",
      "batch 60 accuracy: 0.09375\n",
      "overall Testing Accuracy:  0.05505952380952381\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.run(iterator_test.initializer)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        sess.run(iterator.initializer)\n",
    "        print \"Epoch:\",epoch ,\"running\"\n",
    "        \n",
    "        \n",
    "        for batch in range(batches):\n",
    "            if batch%10!=0:\n",
    "                sess.run(train_op,feed_dict={a:True})\n",
    "            if batch%10==0:\n",
    "                loss,acc,_=sess.run([loss_op,accuracy,train_op],feed_dict={a:True})\n",
    "                print \"batch\",batch,\"loss:\",loss,\"accuracy\",acc\n",
    "            \n",
    "                 \n",
    "                \n",
    "        print \"Epoch:\",epoch,\"ended\"\n",
    "    print \"optimazion finished\"\n",
    "    accuracy_list=[]\n",
    "    \n",
    "    \n",
    "    for batch in range(batches_test):\n",
    "            acc=sess.run(accuracy,feed_dict={a:False})\n",
    "            accuracy_list.append(acc)\n",
    "            if batch%10==0:\n",
    "                print \"batch\",batch,\"accuracy:\",acc\n",
    "    n=sum(accuracy_list)/len(accuracy_list)\n",
    "    print \"overall Testing Accuracy: \",n\n",
    "   \n",
    "   \n",
    "  \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
